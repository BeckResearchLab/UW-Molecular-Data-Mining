{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Elsevier Metadata\n",
    "\n",
    "This notebook is used for pulling metadata from articles via Scopus' literature search. It can technically be used to scrape abstracts from anywhere within Scopus' database, but we've specifically limited it to Elsevier journals as that is the only journal that we have access to the fulltext options from. Specifically, this sets up a way to pull PII identification numbers automatically.\n",
    "\n",
    "To manually test queries, go to https://www.scopus.com/search/form.uri?display=advanced\n",
    "\n",
    "Elsevier maintains a list of all journals in a single excel spreadsheet. The link to that elsevier active journals link: https://www.elsevier.com/__data/promis_misc/sd-content/journals/jnlactivesubject.xls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole of this scraping tool centers around `pybliometrics`, a prebuilt scraping package that interacts with the quirks of Scopus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybliometrics\n",
    "from pybliometrics.scopus import ScopusSearch\n",
    "from pybliometrics.scopus.exception import Scopus429Error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import multiprocessing\n",
    "from os import system, name\n",
    "import json\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from pybliometrics.scopus import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use `pybliometrics`, you have to set up a config file on your computer. The best way to do that is to just use the built in command, `pybliometrics.scopus.utils.create_config()`. It will prompt you to enter an API key, so make sure you have one at the ready before you run this command. You can get one easily from https://dev.elsevier.com/documentation/SCOPUSSearchAPI.wadl with a quick registration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your API Key, obtained from http://dev.elsevier.com/myapikey.html: \n",
      " 646199a6755da12c28f3fdfe59bbfe55\n",
      "API Keys are sufficient for most users.  If you have to use Authtoken authentication, please enter the token, otherwise press Enter: \n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file successfully created at C:\\Users\\Jonathan/.scopus/config.ini\n"
     ]
    }
   ],
   "source": [
    "#In addition to imports, the first time we ever run pybliometrics we need to config pybliometrics\n",
    "#My API key: 646199a6755da12c28f3fdfe59bbfe55\n",
    "#pybliometrics.scopus.utils.create_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should note the above filepath location, as it's important to have this filepath for later function calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note your config path for pybliometrics: C:\\Users\\Jonathan/.scopus/config.ini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get into it - Time to walk through the algorithm!\n",
    "\n",
    "List of things by which the algorithm will parse searches:\n",
    "\n",
    "1. Year\n",
    "2. Journal\n",
    "3. Keyword search\n",
    "\n",
    "So, we'll have to select a set of these parameters to fine-tune our search to get articles that'll be useful to us. \n",
    "\n",
    "One of the first quick parameters that will help is to filter down the number of journals that we'll be searching through, and then organize them into a dataframe so we can continue to work through the data in later methods.\n",
    "\n",
    "We'll first go through all the methods that we have, then we'll show you exactly how to use the methods with some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method, `make_jlist`, creates a dataframe that only contains journals mentioning certain keywords in their 'Full_Category' column. Those keywords are passed directly to the method, though some default keywords can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_jlist(jlist_url = 'https://www.elsevier.com/__data/promis_misc/sd-content/journals/jnlactivesubject.xls', \n",
    "               journal_strings = ['chemistry','energy','molecular','atomic','chemical','biochem'\n",
    "                                  ,'organic','polymer','chemical engineering','biotech','colloid']):\n",
    "    \"\"\"\n",
    "    This method creates a dataframe of relevant journals to query. The dataframe contains two columns:\n",
    "    (1) The names of the Journals\n",
    "    (2) The issns of the Journals\n",
    "    \n",
    "    As inputs, the URL for a journal list and a list of keyword strings to subselect the journals by is required.\n",
    "    These values currently default to Elsevier's journals and some chemical keywords.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This creates a dataframe of the active journals and their subjects from elsevier\n",
    "    active_journals = pd.read_excel(jlist_url)\n",
    "    # This makes the dataframe column names a smidge more intuitive.\n",
    "    active_journals.rename(columns = {'Display Category Full Name':'Full_Category','Full Title':'Journal_Title'}, inplace = True)\n",
    "    \n",
    "    active_journals.Full_Category = active_journals.Full_Category.str.lower() # lowercase topics for searching\n",
    "    active_journals = active_journals.drop_duplicates(subset = 'Journal_Title') # drop any duplicate journals\n",
    "    active_journals = shuffle(active_journals, random_state = 42) \n",
    "\n",
    "    # new dataframe full of only journals who's topic description contained the desired keywords\n",
    "    active_journals = active_journals[active_journals['Full_Category'].str.contains('|'.join(journal_strings))]\n",
    "\n",
    "    #Select down to only the title and the individual identification number called ISSN\n",
    "    journal_frame = active_journals[['Journal_Title','ISSN']]\n",
    "    #Remove things that have were present in multiple name searches.\n",
    "    journal_frame = journal_frame.drop_duplicates(subset = 'Journal_Title')\n",
    "    \n",
    "    return journal_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following method builds the keyword search portion of a query. There is an example below that can be copy-pasted into the Scopus advanced Search.\n",
    "This method is a helper function, and you really shouldn't need to interact with it. It helps to combine several terms in a way that would be unnatural for us to type, but is necessary for online searching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_search_terms(kwds):\n",
    "    \"\"\"\n",
    "    This builds the keyword search portion of the query string. \n",
    "    \"\"\"\n",
    "    combined_keywords = \"\"\n",
    "    for i in range(len(kwds)):\n",
    "        if i != len(kwds)-1:\n",
    "            combined_keywords += kwds[i] + ' OR '\n",
    "        else:\n",
    "            combined_keywords += kwds[i] + ' '\n",
    "    \n",
    "    return combined_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a model test query \n",
    "# test = search(verbose = True, query = 'polymer OR organic OR molecular AND PUBYEAR IS 2019 AND ISSN(00404020)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following method builds the entire query to be put into pybliometrics\n",
    "The query requires a pretty specific format, so we are using a helper function to make it less obnoxious to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query_dict(term_list, issn_list, year_list):\n",
    "    \"\"\"\n",
    "    This method takes the list of journals and creates a nested dictionary\n",
    "    containing all accessible queries, in each year, for each journal,\n",
    "    for a given keyword search on sciencedirect.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    term_list(list, required): the list of search terms looked for in papers by the api.\n",
    "    \n",
    "    issn_list(list, required): the list of journal issn's to be queried. Can be created by getting the '.values'\n",
    "    of a 'journal_list' dataframe that has been created from the 'make_jlist' method.\n",
    "    \n",
    "    year_list(list, required): the list of years which will be searched through\n",
    "    \n",
    "    \"\"\"\n",
    "    search_terms = build_search_terms(term_list)\n",
    "    dict1 = {}\n",
    "    #This loop goes through and sets up a dictionary key with an ISSN number\n",
    "    for issn in issn_list:\n",
    "        \n",
    "        issn_terms = ' AND ISSN(' + issn + ')'\n",
    "        dict2 = {}\n",
    "        #This loop goes and attaches all the years to the outer loop's key.\n",
    "        for year in year_list:\n",
    "            \n",
    "            year_terms = \"AND PUBYEAR IS \" + str(year)\n",
    "            querystring = search_terms + year_terms + issn_terms\n",
    "\n",
    "            dict2[year] = querystring\n",
    "\n",
    "        dict1[issn] = dict2\n",
    "\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we can either run this with a single process, or we can multiprocess our way to victory. Either way, the first thing we need to do is define a set of functions that will follow our list of journals, as well as a set of outlined years, and search for a list of terms within those journals and years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is a method to clear the cache. Doesn't matter too much because 1.1 million pubs stored in cache only took 2 GB of memory \n",
    "BE CAREFUL WITH THIS. IT CAN DELETE EVERYTHING ON YOUR COMPUTER IF YOU MESS IT UP. But it can be useful if your cache starts to get too full and take up too much memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cache(cache_path):\n",
    "    \"\"\"\n",
    "    Be very careful with this method. It can delete your entire computer if you let it. \n",
    "    \"\"\"\n",
    "    \n",
    "    # if the cache path contains the proper substring, and if the files we are deleting are of the propper length, delete the files\n",
    "    \n",
    "    if '.scopus/scopus_search/' in cache_path:\n",
    "        for file in os.listdir(cache_path):\n",
    "            \n",
    "            # Making sure the deleted files match the standard length of pybliometrics cache output\n",
    "            if len(file) == len('8805245317ccb15059e3cfa219be2dd4'):\n",
    "                os.remove(cache_path + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The method below loops through the entire journal list and collects article metadata (ie, not full-text), including PII\n",
    "Unfotunately, collecting fulltext is not possible with this API. We have another method, `Elsevier_fulltext_api.py`, which takes in this metadata information and is able to pull out a full length article. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things we probably want to just grab because we have them:\n",
    "1. Author names\n",
    "2. Author keywords\n",
    "3. Cited by count\n",
    "4. title\n",
    "5. PII\n",
    "6. DOI\n",
    "7. Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_piis(term_list, journal_frame, year_list, cache_path, output_path, keymaster=False, fresh_keys=None, config_path='/Users/DavidCJ/.scopus/config.ini'):\n",
    "    \"\"\"\n",
    "    This should be a standalone method that recieves a list of journals (issns), a keyword search,\n",
    "    an output path and a path to clear the cache. It should be mappable to multiple parallel processes. \n",
    "    \"\"\"\n",
    "    if output_path[-1] is not '/':\n",
    "        raise Exception('Output file path must end with /')\n",
    "    \n",
    "    if '.scopus/scopus_search' not in cache_path:\n",
    "        raise Exception('Cache path is not a sub-directory of the scopus_search. Make sure cache path is correct.')\n",
    "    \n",
    "    # Two lists who's values correspond to each other    \n",
    "    issn_list = journal_frame['ISSN'].values\n",
    "    journal_list = journal_frame['Journal_Title'].values\n",
    "    # Find and replaces slashes and spaces in names for file storage purposes\n",
    "    for j in range(len(journal_list)):\n",
    "        if ':' in journal_list[j]:\n",
    "            journal_list[j] = journal_list[j].replace(':','')\n",
    "        elif '/' in journal_list[j]:\n",
    "            journal_list[j] = journal_list[j].replace('/','_') \n",
    "        elif ' ' in journal_list[j]:\n",
    "            journal_list[j] = journal_list[j].replace(' ','_')\n",
    "    \n",
    "            \n",
    "    \n",
    "    # Build the dictionary that can be used to sequentially query elsevier for different journals and years\n",
    "    query_dict = build_query_dict(term_list,issn_list,year_list)\n",
    "    \n",
    "    # Must write to memory, clear cache, and clear a dictionary upon starting every new journal\n",
    "    for i in range(len(issn_list)):\n",
    "        # At the start of every year, clear the standard output screen\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        paper_counter = 0\n",
    "\n",
    "        issn_dict = {}\n",
    "        for j in range(len(year_list)):\n",
    "            # for every year in every journal, query the keywords\n",
    "            print(f'{journal_list[i]} in {year_list[j]}.')\n",
    "            \n",
    "            # Want the sole 'keymaster' process to handle 429 responses by swapping the key. \n",
    "            if keymaster:\n",
    "                try:\n",
    "                    query_results = ScopusSearch(verbose = True,query = query_dict[issn_list[i]][year_list[j]])\n",
    "                except Scopus429Error:\n",
    "                    print('entered scopus 429 error loop... replacing key')\n",
    "                    newkey = fresh_keys.pop(0)\n",
    "                    config[\"Authentication\"][\"APIKey\"] = newkey\n",
    "                    time.sleep(5)\n",
    "                    query_results = ScopusSearch(verbose = True,query = query_dict[issn_list[i]][year_list[j]])\n",
    "                    print('key swap worked!!')\n",
    "            # If this process isn't the keymaster, try a query. \n",
    "            # If it excepts, wait a few seconds for keymaster to replace key and try again.\n",
    "            else:\n",
    "                try:\n",
    "                    query_results = ScopusSearch(verbose = True,query = query_dict[issn_list[i]][year_list[j]])\n",
    "                except Scopus429Error:\n",
    "                    print('Non key master is sleeping for 15... ')\n",
    "                    time.sleep(15)\n",
    "                    query_results = ScopusSearch(verbose = True,query = query_dict[issn_list[i]][year_list[j]]) # at this point, the scopus 429 error should be fixed... \n",
    "                    print('Non key master slept, query has now worked.')\n",
    "            \n",
    "            # store relevant information from the results into a dictionary pertaining to that query\n",
    "            year_dict = {}\n",
    "            if query_results.results is not None:\n",
    "                # some of the query results might be of type None \n",
    "                \n",
    "                \n",
    "                for k in range(len(query_results.results)):\n",
    "                    paper_counter += 1\n",
    "                    \n",
    "                    result_dict = {}\n",
    "                    result = query_results.results[k]\n",
    "\n",
    "                    result_dict['pii'] = result.pii\n",
    "                    result_dict['doi'] = result.doi\n",
    "                    result_dict['title'] = result.title\n",
    "                    result_dict['num_authors'] = result.author_count\n",
    "                    result_dict['authors'] = result.author_names\n",
    "                    result_dict['description'] = result.description\n",
    "                    result_dict['citation_count'] = result.citedby_count\n",
    "                    result_dict['keywords'] = result.authkeywords\n",
    "                    \n",
    "                    year_dict[k] = result_dict\n",
    "\n",
    "                # Store all of the results for this year in the dictionary containing to a certain journal\n",
    "                issn_dict[year_list[j]] = year_dict\n",
    "            else:\n",
    "                # if it was a None type, we will just store the empty dictionary as json\n",
    "                issn_dict[year_list[j]] = year_dict\n",
    "        \n",
    "        \n",
    "        # Store all of the results for this journal in a folder as json file\n",
    "        os.mkdir(f'{output_path}{journal_list[i]}')\n",
    "        with open(f'{output_path}{journal_list[i]}/{journal_list[i]}.json','w') as file:\n",
    "            json.dump(issn_dict, file)\n",
    "        \n",
    "        with open(f'{output_path}{journal_list[i]}/{journal_list[i]}.txt','w') as file2:\n",
    "            file2.write(f'This file contains {paper_counter} publications.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "## Example Time!\n",
    "\n",
    "Ok, now that we've shown all the methods, let's investigate their usage. We'll walk through linearly, so feel free to use these cells to figure out and run your own scraping efforts.\n",
    "\n",
    "First thing's first, we need to call the `make_jlist` method and pass it anything we want to search by, and receive a dataframe of our downselected set of journals. You will get a warning from this method call, but it's not a big deal. It's an underlying weirdness of the pandas.read_excel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    }
   ],
   "source": [
    "journal_list = make_jlist(jlist_url = 'https://www.elsevier.com/__data/promis_misc/sd-content/journals/jnlactivesubject.xls', \n",
    "               journal_strings = ['chemistry','synthesis','molecular','chemical','organic','polymer','materials'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Journal_Title</th>\n",
       "      <th>ISSN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1574</td>\n",
       "      <td>Current Research in Physiology</td>\n",
       "      <td>26659441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6609</td>\n",
       "      <td>Tetrahedron Letters</td>\n",
       "      <td>00404039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2071</td>\n",
       "      <td>Environmental Nanotechnology, Monitoring &amp; Man...</td>\n",
       "      <td>22151532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2781</td>\n",
       "      <td>Immunobiology</td>\n",
       "      <td>01712985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4695</td>\n",
       "      <td>Matrix Biology</td>\n",
       "      <td>0945053X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Journal_Title      ISSN\n",
       "1574                     Current Research in Physiology  26659441\n",
       "6609                                Tetrahedron Letters  00404039\n",
       "2071  Environmental Nanotechnology, Monitoring & Man...  22151532\n",
       "2781                                      Immunobiology  01712985\n",
       "4695                                     Matrix Biology  0945053X"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print out to show you the structure of the journal dataframe\n",
    "journal_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'polymer OR organic OR molecular OR molecule OR chemistry OR synthesis AND PUBYEAR IS 2015 AND ISSN(00404020)'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of how to use the dictionary builder\n",
    "issn_list = journal_list['ISSN'].values\n",
    "dictionary = build_query_dict(term_list, issn_list, range(1995,2021))\n",
    "#This shows a specific journal ISSN, and specific year selected. \n",
    "dictionary['00404020'][2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of Jon's API Keys - Feel free (please do) to grab your own quickly from the Scopus website. \n",
    "apikeylist = ['646199a6755da12c28f3fdfe59bbfe55','f23e69765c41a3a6e042eb9baf73bd77','f6dafc105b5adfe25105eb658aa80b7c', \n",
    "              '\te9f7c3a33c7bf1b790372d25a8fbb5a1', '2e57cbb3c25fa9e446a8fd0e58be91e9', '1bed2480701164024b1a644843c76099']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we'll go ahead and set up the other search terms and cache paths, so we can run our full `get_piis` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '/Users/Jonathan/.scopus/scopus_search/COMPLETE/'\n",
    "term_list = ['polymer','organic','molecular', 'chemistry', 'synthesis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, with search terms in hand, and a downselected journal list, we're ready to go and scrape papers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_piis(term_list,journal_frame,range(1995,2021),cache_path=cache_path,output_path = '/Users/Jonathan/Desktop/pyblio_test/', keymaster = True, fresh_keys = apikeylist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further development work that isn't quite yet fully done/working?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiprocess(term_list, journal_frame, year_list, cache_path, output_path, keymaster = False, fresh_keys = None, config_path = '/Users/Jonathan/.scopus/config.ini', split_ratio = 2):\n",
    "    \"\"\"asdfoinasdfoin\"\"\"\n",
    "    split_list = np.array_split(journal_frame, split_ratio)\n",
    "    processes = []\n",
    "    for k in range(split_ratio):\n",
    "        print(\"Before multiprocessing\")\n",
    "        p = multiprocessing.Process(target = get_piis, args = [term_list, split_list[k], year_list, cache_path, output_path, keymaster, fresh_keys, config_path])\n",
    "        print(\"after multiprocessing\")\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for process in processes:\n",
    "        process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before multiprocessing\n",
      "after multiprocessing\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4d332acd19b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmultiprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjournal_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1995\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2021\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/Users/Jonathan/Desktop/pyblio_test2/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeymaster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfresh_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfresh_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig_path\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;34m'/Users/Jonathan/.scopus/config.ini'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# split_ratio = 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# split_list = np.array_split(journal_frame, split_ratio)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# for k in range(split_ratio):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#     p = multiprocessing.Process(target = get_piis, args = [term_list,split_list[k],range(1995,2021),cache_path,'/Volumes/My Passport/Davids Stuff/pyblio_test3/', True, fresh_keys])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-9565bddb8d86>\u001b[0m in \u001b[0;36mmultiprocess\u001b[1;34m(term_list, journal_frame, year_list, cache_path, output_path, keymaster, fresh_keys, config_path, split_ratio)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_piis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mterm_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myear_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeymaster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfresh_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"after multiprocessing\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mprocesses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mprocess\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "multiprocess(term_list, journal_frame, range(1995,2021), cache_path=cache_path, output_path = '/Users/Jonathan/Desktop/pyblio_test2/', keymaster = True, fresh_keys = fresh_keys, config_path =  '/Users/Jonathan/.scopus/config.ini', split_ratio = 2)\n",
    "# split_ratio = 3\n",
    "# split_list = np.array_split(journal_frame, split_ratio)\n",
    "# for k in range(split_ratio):\n",
    "#     p = multiprocessing.Process(target = get_piis, args = [term_list,split_list[k],range(1995,2021),cache_path,'/Volumes/My Passport/Davids Stuff/pyblio_test3/', True, fresh_keys])\n",
    "#     p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we need to split our list of journals in half\n",
    "df1, df2 = np.array_split(journal_frame,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-92823fc4e755>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#p4 = multiprocessing.Process(target = get_piis, args = [term_list,df4,range(1995,2021),cache_path,'/Volumes/My Passport/Davids Stuff/pyblio_test2/'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mp1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mp2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#p3.start()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "p1 = multiprocessing.Process(target = get_piis, args = [term_list,df1,range(1995,2021),cache_path,'/Volumes/My Passport/Davids Stuff/pyblio_test3/'])\n",
    "p2 = multiprocessing.Process(target = get_piis, args = [term_list,df2,range(1995,2021),cache_path,'/Volumes/My Passport/Davids Stuff/pyblio_test3/',True,fresh_keys])\n",
    "#p3 = multiprocessing.Process(target = get_piis, args = [term_list,df3,range(1995,2021),cache_path,'/Volumes/My Passport/Davids Stuff/pyblio_test2/'])\n",
    "#p4 = multiprocessing.Process(target = get_piis, args = [term_list,df4,range(1995,2021),cache_path,'/Volumes/My Passport/Davids Stuff/pyblio_test2/'])\n",
    "\n",
    "p1.start()\n",
    "p2.start()\n",
    "#p3.start()\n",
    "#p4.start()\n",
    "\n",
    "# starttime=time.time()\n",
    "# while True:\n",
    "#     clear_cache(cache_path)\n",
    "#     clear_output()\n",
    "#     time.sleep(20.0 - ((time.time() - starttime) % 20.0)) \n",
    "\n",
    "p1.join()\n",
    "p2.join()\n",
    "#p3.join()\n",
    "#p4.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff below is for counting how many publications are located in an output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absoluteFilePaths(directory):\n",
    "    for dirpath,_,filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            yield os.path.abspath(os.path.join(dirpath, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = open('/Volumes/My Passport/Davids Stuff/pyblio_test/Gene: X/Gene: X.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pubs(output_path):\n",
    "    count = 0\n",
    "    for path in absoluteFilePaths(output_path):\n",
    "        if 'txt' in path and '._' not in path:\n",
    "            file = open(path,'r')\n",
    "            #print(path)\n",
    "            a = sum([int(s) for s in string.split() if s.isdigit()])\n",
    "            count+=a\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e7300121dd83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcount_pubs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Users/Jonathan/Desktop/pyblio_test/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-9014e62b5e1a>\u001b[0m in \u001b[0;36mcount_pubs\u001b[1;34m(output_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;31m#print(path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "count_pubs('/Users/Jonathan/Desktop/pyblio_test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
