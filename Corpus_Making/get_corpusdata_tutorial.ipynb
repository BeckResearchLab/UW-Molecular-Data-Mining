{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping MetaData and getting Fulltexts\n",
    "\n",
    "This tutorial notebook uses the functions from `pybliometrics` and `Elsevier_fulltext_api` and shows how to get the fulltexts from the articles you pulled from Scopus.\n",
    "\n",
    "The first part of the notebook is used for pulling metadata from articles via Scopus' literature search. It can technically be used to scrape abstracts from anywhere within Scopus' database, but we've specifically limited it to Elsevier journals as that is the only journal that we have access to the fulltext options from. Specifically, this sets up a way to pull PII identification numbers automatically.\n",
    "\n",
    "To manually test queries, go to https://www.scopus.com/search/form.uri?display=advanced\n",
    "\n",
    "Elsevier maintains a list of all journals in a single excel spreadsheet. The link to that elsevier active journals link: https://www.elsevier.com/__data/promis_misc/sd-content/journals/jnlactivesubject.xls\n",
    "\n",
    "The second part of the notebook uses the metadata generated from the first part and gets the fulltexts out of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybliometrics\n",
    "from pybliometrics.scopus import ScopusSearch\n",
    "from pybliometrics.scopus.exception import Scopus429Error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import multiprocessing\n",
    "from os import system, name\n",
    "import json\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from pybliometrics.scopus import config\n",
    "from elsapy.elsclient import ElsClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the articles, the first step requires you to get an API key from Scopus and adding it to your local config file. You can easily get an API key from https://dev.elsevier.com/documentation/SCOPUSSearchAPI.wadl with a quick registration. \n",
    "\n",
    "Once you have your API key, you need to add it to your computer using the following command:\n",
    "\n",
    "`import pybliometrics`\n",
    "\n",
    "`pybliometrics.scopus.utils.create_config()`\n",
    "\n",
    "This will prompt you to enter an API key which you obtained from the Scopus website. Once you're done with that you are good to download the articles using the following functions.\n",
    "\n",
    "**Note**: While downloading the articles from the Scopus, make sure you are connected to UW VPN (All Internet Traffic) using the BIG-IP Edge Client. Without that you might end up getting the Scopus authorization error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config path for `pybliometrics` is: `/Users/nisarg/.scopus/config.ini` (Would vary as per your local path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get into it - Time to walk through the algorithm!\n",
    "\n",
    "List of things by which the algorithm will parse searches:\n",
    "\n",
    "1. Year\n",
    "2. Journal\n",
    "3. Keyword search\n",
    "\n",
    "So, we'll have to select a set of these parameters to fine-tune our search to get articles that'll be useful to us. \n",
    "\n",
    "One of the first quick parameters that will help is to filter down the number of journals that we'll be searching through, and then organize them into a dataframe so we can continue to work through the data in later methods.\n",
    "\n",
    "We'll first go through all the methods that we have, then we'll show you exactly how to use the methods with some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method, `make_jlist`, creates a dataframe that only contains journals mentioning certain keywords in their 'Full_Category' column. Those keywords are passed directly to the method, though some default keywords can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_jlist(jlist_url = 'https://www.elsevier.com/__data/promis_misc/sd-content/journals/jnlactivesubject.xls', \n",
    "               journal_strings = ['chemistry','energy','molecular','atomic','chemical','biochem'\n",
    "                                  ,'organic','polymer','chemical engineering','biotech','colloid']):\n",
    "    \"\"\"\n",
    "    This method creates a dataframe of relevant journals to query. The dataframe contains two columns:\n",
    "    (1) The names of the Journals\n",
    "    (2) The issns of the Journals\n",
    "    \n",
    "    As inputs, the URL for a journal list and a list of keyword strings to subselect the journals by is required.\n",
    "    These values currently default to Elsevier's journals and some chemical keywords.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This creates a dataframe of the active journals and their subjects from elsevier\n",
    "    active_journals = pd.read_excel(jlist_url)\n",
    "    # This makes the dataframe column names a smidge more intuitive.\n",
    "    active_journals.rename(columns = {'Display Category Full Name':'Full_Category','Full Title':'Journal_Title'}, inplace = True)\n",
    "    \n",
    "    active_journals.Full_Category = active_journals.Full_Category.str.lower() # lowercase topics for searching\n",
    "    active_journals = active_journals.drop_duplicates(subset = 'Journal_Title') # drop any duplicate journals\n",
    "    active_journals = shuffle(active_journals, random_state = 42) \n",
    "\n",
    "    # new dataframe full of only journals who's topic description contained the desired keywords\n",
    "    active_journals = active_journals[active_journals['Full_Category'].str.contains('|'.join(journal_strings))]\n",
    "\n",
    "    #Select down to only the title and the individual identification number called ISSN\n",
    "    journal_frame = active_journals[['Journal_Title','ISSN']]\n",
    "    #Remove things that have were present in multiple name searches.\n",
    "    journal_frame = journal_frame.drop_duplicates(subset = 'Journal_Title')\n",
    "    \n",
    "    return journal_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following method builds the keyword search portion of a query. There is an example below that can be copy-pasted into the Scopus advanced Search.\n",
    "This method is a helper function, and you really shouldn't need to interact with it. It helps to combine several terms in a way that would be unnatural for us to type, but is necessary for online searching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_search_terms(kwds):\n",
    "    \"\"\"\n",
    "    This builds the keyword search portion of the query string. \n",
    "    \"\"\"\n",
    "    combined_keywords = \"\"\n",
    "    for i in range(len(kwds)):\n",
    "        if i != len(kwds)-1:\n",
    "            combined_keywords += kwds[i] + ' OR '\n",
    "        else:\n",
    "            combined_keywords += kwds[i] + ' '\n",
    "    \n",
    "    return combined_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following method builds the entire query to be put into pybliometrics\n",
    "The query requires a pretty specific format, so we are using a helper function to make it less obnoxious to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query_dict(term_list, issn_list, year_list):\n",
    "    \"\"\"\n",
    "    This method takes the list of journals and creates a nested dictionary\n",
    "    containing all accessible queries, in each year, for each journal,\n",
    "    for a given keyword search on sciencedirect.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    term_list(list, required): the list of search terms looked for in papers by the api.\n",
    "    \n",
    "    issn_list(list, required): the list of journal issn's to be queried. Can be created by getting the '.values'\n",
    "    of a 'journal_list' dataframe that has been created from the 'make_jlist' method.\n",
    "    \n",
    "    year_list(list, required): the list of years which will be searched through\n",
    "    \n",
    "    \"\"\"\n",
    "    search_terms = build_search_terms(term_list)\n",
    "    dict1 = {}\n",
    "    #This loop goes through and sets up a dictionary key with an ISSN number\n",
    "    for issn in issn_list:\n",
    "        \n",
    "        issn_terms = ' AND ISSN(' + issn + ')'\n",
    "        dict2 = {}\n",
    "        #This loop goes and attaches all the years to the outer loop's key.\n",
    "        for year in year_list:\n",
    "            \n",
    "            year_terms = \"AND PUBYEAR IS \" + str(year)\n",
    "            querystring = search_terms + year_terms + issn_terms\n",
    "\n",
    "            dict2[year] = querystring\n",
    "\n",
    "        dict1[issn] = dict2\n",
    "\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method shows how to collect the article metadata including the PII by looping through the journal of articles available for our `term_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_piis(term_list, journal_frame, year_list, cache_path, output_path, keymaster=False, fresh_keys=None, config_path='/Users/nisarg/.scopus/config.ini'):\n",
    "    \"\"\"\n",
    "    This should be a standalone method that recieves a list of journals (issns), a keyword search,\n",
    "    an output path and a path to clear the cache. It should be mappable to multiple parallel processes. \n",
    "    \"\"\"\n",
    "    if output_path[-1] is not '/':\n",
    "        raise Exception('Output file path must end with /')\n",
    "    \n",
    "    if '.scopus/scopus_search' not in cache_path:\n",
    "        raise Exception('Cache path is not a sub-directory of the scopus_search. Make sure cache path is correct.')\n",
    "    \n",
    "    # Two lists who's values correspond to each other    \n",
    "    issn_list = journal_frame['ISSN'].values\n",
    "    journal_list = journal_frame['Journal_Title'].values\n",
    "    # Find and replaces slashes and spaces in names for file storage purposes\n",
    "    for j in range(len(journal_list)):\n",
    "        if ':' in journal_list[j]:\n",
    "            journal_list[j] = journal_list[j].replace(':','')\n",
    "        elif '/' in journal_list[j]:\n",
    "            journal_list[j] = journal_list[j].replace('/','_') \n",
    "        elif ' ' in journal_list[j]:\n",
    "            journal_list[j] = journal_list[j].replace(' ','_')\n",
    "    \n",
    "            \n",
    "    \n",
    "    # Build the dictionary that can be used to sequentially query elsevier for different journals and years\n",
    "    query_dict = build_query_dict(term_list,issn_list,year_list)\n",
    "    \n",
    "    # Must write to memory, clear cache, and clear a dictionary upon starting every new journal\n",
    "    for i in range(len(issn_list)):\n",
    "        # At the start of every year, clear the standard output screen\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        paper_counter = 0\n",
    "\n",
    "        issn_dict = {}\n",
    "        for j in range(len(year_list)):\n",
    "            # for every year in every journal, query the keywords\n",
    "            print(f'{journal_list[i]} in {year_list[j]}.')\n",
    "            \n",
    "            # Want the sole 'keymaster' process to handle 429 responses by swapping the key. \n",
    "            if keymaster:\n",
    "                try:\n",
    "                    query_results = ScopusSearch(verbose = True,query = query_dict[issn_list[i]][year_list[j]])\n",
    "                except Scopus429Error:\n",
    "                    print('entered scopus 429 error loop... replacing key')\n",
    "                    newkey = fresh_keys.pop(0)\n",
    "                    config[\"Authentication\"][\"APIKey\"] = newkey\n",
    "                    time.sleep(5)\n",
    "                    query_results = ScopusSearch(verbose = True,query = query_dict[issn_list[i]][year_list[j]])\n",
    "                    print('key swap worked!!')\n",
    "            # If this process isn't the keymaster, try a query. \n",
    "            # If it excepts, wait a few seconds for keymaster to replace key and try again.\n",
    "            else:\n",
    "                try:\n",
    "                    query_results = ScopusSearch(verbose = True,query = query_dict[issn_list[i]][year_list[j]])\n",
    "                except Scopus429Error:\n",
    "                    print('Non key master is sleeping for 15... ')\n",
    "                    time.sleep(15)\n",
    "                    query_results = ScopusSearch(verbose = True,query = query_dict[issn_list[i]][year_list[j]]) # at this point, the scopus 429 error should be fixed... \n",
    "                    print('Non key master slept, query has now worked.')\n",
    "            \n",
    "            # store relevant information from the results into a dictionary pertaining to that query\n",
    "            year_dict = {}\n",
    "            if query_results.results is not None:\n",
    "                # some of the query results might be of type None \n",
    "                \n",
    "                \n",
    "                for k in range(len(query_results.results)):\n",
    "                    paper_counter += 1\n",
    "                    \n",
    "                    result_dict = {}\n",
    "                    result = query_results.results[k]\n",
    "\n",
    "                    result_dict['pii'] = result.pii\n",
    "                    result_dict['doi'] = result.doi\n",
    "                    result_dict['title'] = result.title\n",
    "                    result_dict['num_authors'] = result.author_count\n",
    "                    result_dict['authors'] = result.author_names\n",
    "                    result_dict['description'] = result.description\n",
    "                    result_dict['citation_count'] = result.citedby_count\n",
    "                    result_dict['keywords'] = result.authkeywords\n",
    "                    \n",
    "                    year_dict[k] = result_dict\n",
    "\n",
    "                # Store all of the results for this year in the dictionary containing to a certain journal\n",
    "                issn_dict[year_list[j]] = year_dict\n",
    "            else:\n",
    "                # if it was a None type, we will just store the empty dictionary as json\n",
    "                issn_dict[year_list[j]] = year_dict\n",
    "        \n",
    "        \n",
    "        # Store all of the results for this journal in a folder as json file\n",
    "        os.mkdir(f'{output_path}{journal_list[i]}')\n",
    "        with open(f'{output_path}{journal_list[i]}/{journal_list[i]}.json','w') as file:\n",
    "            json.dump(issn_dict, file)\n",
    "        \n",
    "        with open(f'{output_path}{journal_list[i]}/{journal_list[i]}.txt','w') as file2:\n",
    "            file2.write(f'This file contains {paper_counter} publications.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for getting the PII and the metadata from the journals\n",
    "\n",
    "\n",
    "First thing's first, we need to call the `make_jlist` method and pass it anything we want to search by, and receive a dataframe of our downselected set of journals. You will get a warning from this method call, but it's not a big deal. It's an underlying weirdness of the pandas.read_excel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    }
   ],
   "source": [
    "journal_frame = make_jlist(jlist_url = 'https://www.elsevier.com/__data/promis_misc/sd-content/journals/jnlactivesubject.xls', \n",
    "               journal_strings = ['chemistry','synthesis','molecular','chemical','organic','polymer','materials'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Journal_Title</th>\n",
       "      <th>ISSN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6506</th>\n",
       "      <td>Solid State Communications</td>\n",
       "      <td>00381098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>Forensic Chemistry</td>\n",
       "      <td>24681709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4086</th>\n",
       "      <td>Journal of Molecular and Cellular Cardiology</td>\n",
       "      <td>00222828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>Journal of Molecular Spectroscopy</td>\n",
       "      <td>00222852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>Biophysical Chemistry</td>\n",
       "      <td>03014622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Journal_Title      ISSN\n",
       "6506                    Solid State Communications  00381098\n",
       "2451                            Forensic Chemistry  24681709\n",
       "4086  Journal of Molecular and Cellular Cardiology  00222828\n",
       "4098             Journal of Molecular Spectroscopy  00222852\n",
       "707                          Biophysical Chemistry  03014622"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journal_frame.head() #This shows the journal titles and their ISSN from where we will get the metadata for the articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the `cache_path` which will store the cache and `term_list` which takes the list of the terms we need for searching the articles. \n",
    "\n",
    "To clear the cache, you can find the `clear_cache` function in the `pybliometrics` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '/Users/nisarg/.scopus/scopus_search/COMPLETE/'\n",
    "term_list = ['deposition', 'corrosion', 'inhibit', 'corrosive', 'resistance', 'protect', 'acid', 'base', 'coke', 'coking', 'anti', \\\n",
    "             'layer', 'steel', 'mild steel', 'coating', 'degradation', 'oxidation', \\\n",
    "             'film', 'photo-corrosion', 'hydrolysis', 'Schiff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deposition OR corrosion OR inhibit OR corrosive OR resistance OR protect OR acid OR base OR coke OR coking OR anti OR layer OR steel OR mild steel OR coating OR degradation OR oxidation OR film OR photo-corrosion OR hydrolysis OR Schiff AND PUBYEAR IS 2015 AND ISSN(00404020)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of how to use the dictionary builder\n",
    "issn_list = journal_frame['ISSN'].values\n",
    "dictionary = build_query_dict(term_list, issn_list, range(1995,2021))\n",
    "#This shows a specific journal ISSN, and specific year selected. \n",
    "dictionary['00404020'][2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here you'll need to add the API keys which you generated following the steps mentioned at the beginning of the notebook\n",
    "apikeylist = ['6bcdddd0c63296684f85245fe26ef03d','060a5b0160e1ecc6b361060633700981','28fff643126ba570a7a6315537bb9dde', \n",
    "              '095d720842e4a6103e699e2913da406f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll run the function and get the piis.\n",
    "get_piis(term_list,journal_frame,range(1995,2021),cache_path=cache_path, \\\n",
    "         output_path = '/Users/nisarg/Desktop/summer research/test/', keymaster = True, fresh_keys = apikeylist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the fresh `PII` generated using our function. The next part of the notebook shows how to obtain FullTexts using those piis.\n",
    "\n",
    "The functions below are obtained from `Elsevier_fulltext_api` notebook. The functions show how to obtain the data from the metacorpus path and obtain the Fulltexts for the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_journal_json(absolute_path):\n",
    "    \"\"\"\n",
    "    This method loads data collected on a single journal by the pybliometrics metadata collection module into a dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    absolute_path(str, required) - The path to the .json file containing metadata procured by the pybliometrics module. \n",
    "    \"\"\"\n",
    "    with open(absolute_path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(dtype,identity):\n",
    "    \"\"\"\n",
    "    This method retrieves a 'Doc' object from the Elsevier API. The doc object contains metadata and full-text information\n",
    "    about a publication associated with a given PII. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dtype(str,required): The type of identification string being used to access the document. (Almost always PII in our case.)\n",
    "    \n",
    "    identity: The actual identification string/ PII that will be used to query. \n",
    "    \"\"\"\n",
    "    if dtype == 'pii':\n",
    "        doc = FullDoc(sd_pii = identity)\n",
    "    elif dtype == 'doi':\n",
    "        doc= FullDoc(doi = identity)\n",
    "       \n",
    "    if doc.read(client):\n",
    "            #print (\"doc.title: \", doc.title)\n",
    "            doc.write()   \n",
    "    else:\n",
    "        print (\"Read document failed.\")\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docdata(doc):\n",
    "    \"\"\"\n",
    "    This method attempts to get certain pieces of metadata from an elsapy doc object. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    doc(elsapy object, required): elsapy doc object being searched for\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    text(str): The full text from the original publciation.\n",
    "    \n",
    "    auths(list): The list of authors from the publication.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = doc.data['originalText']                          # grab original full text                                                        \n",
    "    except:\n",
    "        text = 'no text in doc'\n",
    "    \n",
    "    try:\n",
    "        auths = authorize(doc) # a list of authors\n",
    "    except:\n",
    "        auths = []\n",
    "    \n",
    "    return text, auths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authorize(doc):\n",
    "    #this method takes a doc object and returns a list of authors for the doc\n",
    "    auths = []\n",
    "    for auth in doc.data['coredata']['dc:creator']:\n",
    "        auths.append(auth['$'])\n",
    "    \n",
    "    return auths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fulltexts(directory_list, directory_path, output_path, pnum):\n",
    "    \"\"\"\n",
    "    This method takes a list of directories containing 'meta' corpus information from the pybliometrics module and adds full-text information to those files. \n",
    "    \n",
    "    Parameters:\n",
    "    ___________\n",
    "    directory_list(list, required): A list of directories which this method will enter and add full-text information to.\n",
    "    \n",
    "    output_path(str, required): The folder in which the new full text corpus will be placed. \n",
    "    \n",
    "    api_keys(list, required): A list of valid API keys from Elsevier developer. One key needed per process being started.\n",
    "    \"\"\"\n",
    "    #client = client\n",
    "    \n",
    "    for directory in directory_list:\n",
    "        os.mkdir(f'{output_path}/{directory}')\n",
    "        marker = open(f'{output_path}/{directory}/marker.txt','w') # put a file in the directory that lets us know we've been in that directory\n",
    "        marker.close()\n",
    "        \n",
    "        info = open(f'{output_path}/{directory}/info.csv','w') # a file to keep track of errors\n",
    "        info.write('type,file,year,pub') # header\n",
    "        \n",
    "        #print(f'made marker and errors in {directory}')\n",
    "        \n",
    "        \n",
    "        json_file = f'{directory_path}/{directory}/{directory}.json'\n",
    "        j_dict = load_journal_json(json_file) # now we have a dictionary of information in our hands. Access it via journal_dict['year']['pub_number']\n",
    "        rem_list = ['num_authors', 'description', 'citation_count', 'keywords']\n",
    "        for year in j_dict:\n",
    "\n",
    "            if j_dict[year] is not {}:\n",
    "                for pub in j_dict[year]:\n",
    "                    \n",
    "                    pii = j_dict[year][pub]['pii'] # the pii identification number used to get the full text\n",
    "\n",
    "                    try:\n",
    "                        \n",
    "                        doc = get_doc('pii',pii) # don't know if doc retrieval will fail\n",
    "                        print(f'Process {pnum} got doc for {directory}, {year}')\n",
    "                    except Exception as e:\n",
    "                        print(f'EXCEPTION: DOC RETRIEVAL. Process {pnum}')\n",
    "                        print(f'Exception was {e}')\n",
    "                        doc = None \n",
    "                        info.write(f'doc retrieval,{json_file},{year},{pub}')\n",
    "\n",
    "                    text, auths = get_docdata(doc) # doesn't crash even if doc = None\n",
    "                    \n",
    "\n",
    "                    if text is 'no text in doc':\n",
    "                        info.write(f'no text in doc,{json_file},{year},{pub}')\n",
    "                    elif auths is []:\n",
    "                        info.write(f'no auths in doc,{json_file},{year},{pub}')\n",
    "\n",
    "                    j_dict[year][pub]['authors'] = auths\n",
    "                    j_dict[year][pub]['fulltext'] = text # the real magic\n",
    "                    \n",
    "                    for key in rem_list:\n",
    "                        j_dict[year][pub].pop(key)\n",
    "                                        \n",
    "            else:\n",
    "                # the year was empty\n",
    "                info.write(f'year empty,{json_file},{year},{np.nan}')\n",
    "                \n",
    "        info.close()\n",
    "        j_file = f'{output_path}/{directory}/{directory}.json'\n",
    "        \n",
    "        with open(j_file,'w') as file:\n",
    "            json.dump(j_dict,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for obtain Fulltexts using the pii of the journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory path is basically your output path for the get_pii function. \n",
    "#It stores the metadata files for the journals. Output path is where you want your Fulltexts to be downloaded\n",
    "directory_path = '/Users/nisarg/Desktop/summer research/CI_pii'\n",
    "output_path = '/Users/nisarg/Desktop/summer research/CI_fulltexts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ElsClient('6bcdddd0c63296684f85245fe26ef03d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files is the list of all the journals which exist in your directory_path\n",
    "files = os.listdir('/Users/nisarg/Desktop/summer research/CI_pii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have everything you need to obtain the Fulltexts, so we can just use the function and get the Fulltexts we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fulltexts(files, directory_path, output_path, apikeylist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
